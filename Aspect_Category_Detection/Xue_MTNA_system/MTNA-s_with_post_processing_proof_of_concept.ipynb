{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook shows a proof of concept for a post-processing modification to the MTNA-s system.\n",
    "This is quite an unstable proposition, howver, since the output of the NN is not consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T14:46:07.383469Z",
     "start_time": "2020-07-02T14:46:07.373140Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  start here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T14:45:21.331502Z",
     "start_time": "2020-07-02T14:45:21.296448Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pickle.load(open('gold_for_bi_lstm.pkl', 'rb'))\n",
    "df_train = pickle.load(open('train_for_bi_lstm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T14:45:22.342372Z",
     "start_time": "2020-07-02T14:45:22.330972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define documents - these are lemmatised texts in df_train and df_gold\n",
    "docs = df_train.lemmatised.tolist()\n",
    "test_docs = df_test.lemmatised.tolist()\n",
    "# finds the max sentence length\n",
    "len(max(docs, key = lambda i: len(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T14:45:28.955473Z",
     "start_time": "2020-07-02T14:45:28.883550Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize texts\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "encoded_test_docs = t.texts_to_sequences(test_docs)\n",
    "# print(encoded_docs)\n",
    "# pad documents to a max length of 80 words\n",
    "max_length = 79\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post', truncating = 'post')\n",
    "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T15:10:57.737552Z",
     "start_time": "2020-07-02T15:10:29.819703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path = 'E:/Study/Vu/Machine learning experiments/glove.6B/glove.6B.200d.txt'\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open(path, encoding = 'utf-8')\n",
    "# f = open('/media/peter/BigExternal/Study/Vu/Machine learning experiments/glove.6B/glove.6B.200d.txt', encoding = 'utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(f'Loaded {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T15:11:02.795994Z",
     "start_time": "2020-07-02T15:11:02.785567Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 200))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5 classifiers - 1 per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T06:59:04.890952Z",
     "start_time": "2020-07-02T06:59:04.888448Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = df_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T15:11:14.766681Z",
     "start_time": "2020-07-02T15:11:14.739048Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = df_test[['text', 'category0', 'category1','category2','category3']]\n",
    "\n",
    "df_test['cat_ambience'] = 0\n",
    "df_test['cat_anecdotes/miscellaneous'] = 0\n",
    "df_test['cat_food'] = 0\n",
    "df_test['cat_price'] = 0\n",
    "df_test['cat_service'] = 0\n",
    "\n",
    "\n",
    "df_test.loc[(df_test['category0'] == 'ambience') | (df_test['category1'] == 'ambience') | (df_test['category2'] == 'ambience')| (df_test['category3'] == 'ambience'),'cat_ambience'] = 1\n",
    "df_test.loc[(df_test['category0'] == 'anecdotes/miscellaneous') | (df_test['category1'] == 'anecdotes/miscellaneous') | (df_test['category2'] == 'anecdotes/miscellaneous')| (df_test['category3'] == 'anecdotes/miscellaneous'),'cat_anecdotes/miscellaneous'] = 1\n",
    "df_test.loc[(df_test['category0'] == 'food') | (df_test['category1'] == 'food') | (df_test['category2'] == 'food')| (df_test['category3'] == 'food'),'cat_food'] = 1\n",
    "df_test.loc[(df_test['category0'] == 'price') | (df_test['category1'] == 'price') | (df_test['category2'] == 'price')| (df_test['category3'] == 'price'),'cat_price'] = 1\n",
    "df_test.loc[(df_test['category0'] == 'service') | (df_test['category1'] == 'service') | (df_test['category2'] == 'service')| (df_test['category3'] == 'service'),'cat_service'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T15:11:21.830802Z",
     "start_time": "2020-07-02T15:11:21.786701Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train[['text', 'category0', 'category1','category2','category3']]\n",
    "df_train['cat_ambience'] = 0\n",
    "df_train['cat_anecdotes/miscellaneous'] = 0\n",
    "df_train['cat_food'] = 0\n",
    "df_train['cat_price'] = 0\n",
    "\n",
    "df_train['cat_service'] = 0\n",
    "df_train.loc[(df_train['category0'] == 'ambience') | (df_train['category1'] == 'ambience') | (df_train['category2'] == 'ambience')| (df_train['category3'] == 'ambience'),'cat_ambience'] = 1\n",
    "df_train.loc[(df_train['category0'] == 'anecdotes/miscellaneous') | (df_train['category1'] == 'anecdotes/miscellaneous') | (df_train['category2'] == 'anecdotes/miscellaneous')| (df_train['category3'] == 'anecdotes/miscellaneous'),'cat_anecdotes/miscellaneous'] = 1\n",
    "df_train.loc[(df_train['category0'] == 'food') | (df_train['category1'] == 'food') | (df_train['category2'] == 'food')| (df_train['category3'] == 'food'),'cat_food'] = 1\n",
    "\n",
    "df_train.loc[(df_train['category0'] == 'price') | (df_train['category1'] == 'price') | (df_train['category2'] == 'price')| (df_train['category3'] == 'price'),'cat_price'] = 1\n",
    "df_train.loc[(df_train['category0'] == 'service') | (df_train['category1'] == 'service') | (df_train['category2'] == 'service')| (df_train['category3'] == 'service'),'cat_service'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functional api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without concatenated cnn - for compars (window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T17:54:36.384300Z",
     "start_time": "2020-07-02T17:54:35.732497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 79)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 79, 200)      756400      main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 79, 600)      1202400     embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 79, 100)      300100      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 600)          0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 100)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 700)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            1402        concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,260,302\n",
      "Trainable params: 2,260,302\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Input, Embedding, LSTM, Dense, merge, Conv1D, GlobalMaxPooling1D, Concatenate, concatenate\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(79,), name='main_input')\n",
    "\n",
    "x = Embedding(output_dim=200, input_dim=vocab_size, input_length=79,weights=[embedding_matrix],trainable=True)(main_input)\n",
    "lstm_out = Bidirectional(LSTM(units = 300, activation = 'tanh', dropout = 0.2, recurrent_dropout = 0.2, return_sequences = True))(x)\n",
    "\n",
    "cnn5_in = Conv1D(filters = 100, kernel_size = 5, padding='same', activation='tanh',strides=1,input_shape = (None, 300))(lstm_out)\n",
    "cnn5_max_out = GlobalMaxPooling1D()(cnn5_in)\n",
    "bi_max_out = GlobalMaxPooling1D()(lstm_out)\n",
    "merge_one = concatenate([bi_max_out, cnn5_max_out])\n",
    "final_out = Dense(2, activation='softmax')(merge_one)\n",
    "model = keras.Model(inputs=main_input, outputs=final_out)\n",
    "\n",
    "model.compile(optimizer='Adadelta', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modified with post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T17:57:48.202734Z",
     "start_time": "2020-07-02T17:54:36.384300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambience \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       682\n",
      "           1       0.91      0.61      0.73       118\n",
      "\n",
      "    accuracy                           0.93       800\n",
      "   macro avg       0.92      0.80      0.85       800\n",
      "weighted avg       0.93      0.93      0.93       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs, df_train['cat_ambience'].tolist(), epochs=20, validation_split=0.2, verbose = False)\n",
    "output_ambience = model.predict(padded_test_docs)\n",
    "system_predictions_amb = pd.DataFrame(np.argmax(output_ambience,axis=1), columns=['Predicted'])\n",
    "print('ambience', '\\n',classification_report(df_test['cat_ambience'].tolist(), system_predictions_amb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:01:00.066709Z",
     "start_time": "2020-07-02T17:57:48.204749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anecdotes/miscellaneous \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.89       566\n",
      "           1       0.90      0.46      0.61       234\n",
      "\n",
      "    accuracy                           0.83       800\n",
      "   macro avg       0.86      0.72      0.75       800\n",
      "weighted avg       0.84      0.83      0.81       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs, df_train['cat_anecdotes/miscellaneous'].tolist(), epochs=20, validation_split=0.2, verbose = False)\n",
    "output_anec = model.predict(padded_test_docs)\n",
    "system_predictions_anec = pd.DataFrame(np.argmax(output_anec,axis=1), columns=['Predicted'])\n",
    "print('anecdotes/miscellaneous', '\\n',classification_report(df_test['cat_anecdotes/miscellaneous'].tolist(), system_predictions_anec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:04:12.090824Z",
     "start_time": "2020-07-02T18:01:00.066709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       382\n",
      "           1       0.93      0.91      0.92       418\n",
      "\n",
      "    accuracy                           0.92       800\n",
      "   macro avg       0.91      0.92      0.91       800\n",
      "weighted avg       0.92      0.92      0.92       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs, df_train['cat_food'].tolist(), epochs=20, validation_split=0.2, verbose = False)\n",
    "output_food = model.predict(padded_test_docs)\n",
    "system_predictions_food = pd.DataFrame(np.argmax(output_food,axis=1), columns=['Predicted'])\n",
    "print('food', '\\n',classification_report(df_test['cat_food'].tolist(), system_predictions_food))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:07:23.756895Z",
     "start_time": "2020-07-02T18:04:12.090824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       717\n",
      "           1       0.94      0.71      0.81        83\n",
      "\n",
      "    accuracy                           0.96       800\n",
      "   macro avg       0.95      0.85      0.89       800\n",
      "weighted avg       0.96      0.96      0.96       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs, df_train['cat_price'].tolist(), epochs=20, validation_split=0.2, verbose = False)\n",
    "output_price = model.predict(padded_test_docs)\n",
    "system_predictions_price = pd.DataFrame(np.argmax(output_price,axis=1), columns=['Predicted'])\n",
    "print('price', '\\n',classification_report(df_test['cat_price'].tolist(), system_predictions_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:10:31.633099Z",
     "start_time": "2020-07-02T18:07:23.756895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       628\n",
      "           1       0.95      0.84      0.89       172\n",
      "\n",
      "    accuracy                           0.96       800\n",
      "   macro avg       0.95      0.92      0.93       800\n",
      "weighted avg       0.96      0.96      0.96       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs, df_train['cat_service'].tolist(), epochs=20, validation_split=0.2, verbose = False)\n",
    "output_service = model.predict(padded_test_docs)\n",
    "system_predictions_service = pd.DataFrame(np.argmax(output_service,axis=1), columns=['Predicted'])\n",
    "print('service', '\\n',classification_report(df_test['cat_service'].tolist(), system_predictions_service))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list output where all categories = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:10:31.639159Z",
     "start_time": "2020-07-02T18:10:31.633099Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['pred_amb'] = list(system_predictions_amb.Predicted)\n",
    "df_test['pred_anec']= list(system_predictions_anec.Predicted)\n",
    "df_test['pred_food']= list(system_predictions_food.Predicted)\n",
    "df_test['pred_price']= list(system_predictions_price.Predicted)\n",
    "df_test['pred_service'] = list(system_predictions_service.Predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:10:31.662625Z",
     "start_time": "2020-07-02T18:10:31.640181Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nolabel = df_test.loc[(df_test.cat_ambience == 0) & (df_test.pred_anec == 0) & (df_test.pred_food == 0) & (df_test.pred_price == 0) & (df_test.pred_service == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:10:31.667630Z",
     "start_time": "2020-07-02T18:10:31.664627Z"
    }
   },
   "outputs": [],
   "source": [
    "nolabel_list = (list(df_nolabel.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:10:31.672634Z",
     "start_time": "2020-07-02T18:10:31.669632Z"
    }
   },
   "outputs": [],
   "source": [
    "proba_dict = {'ambience':output_ambience, 'anecdotes/miscellaneous':output_anec, 'food':output_food,\n",
    "              'price':output_price,'service':output_service}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T18:10:31.917404Z",
     "start_time": "2020-07-02T18:10:31.673635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted = : ambience \t \t \t actual = : 0\n",
      "predicted = : ambience \t \t \t actual = : 0\n",
      "predicted = : ambience \t \t \t actual = : 0\n",
      "predicted = : ambience \t \t \t actual = : 0\n",
      "predicted = : anecdotes/miscellaneous \t \t \t actual = : 1\n",
      "predicted = : anecdotes/miscellaneous \t \t \t actual = : 1\n",
      "predicted = : anecdotes/miscellaneous \t \t \t actual = : 1\n",
      "predicted = : food \t \t \t actual = : 1\n",
      "predicted = : service \t \t \t actual = : 1\n",
      "predicted = : service \t \t \t actual = : 1\n",
      "predicted = : service \t \t \t actual = : 0\n"
     ]
    }
   ],
   "source": [
    "for key, srs in proba_dict.items():\n",
    "    for idx in nolabel_list:\n",
    "        for i, ls in enumerate(srs):                \n",
    "            if i == int(idx) and ls[1]>0.4:\n",
    "                name = key\n",
    "                print ('predicted = :', name, '\\t', '\\t', '\\t', 'actual = :', df_test[f'cat_{name}'].iloc[i])                                                                                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
